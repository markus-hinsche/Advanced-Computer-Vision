{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e626f0",
   "metadata": {},
   "source": [
    "# Region-based Convolutional Neural Network (R-CNN)\n",
    "\n",
    "<br><br>\n",
    "![](https://lilianweng.github.io/lil-log/assets/images/RCNN.png)\n",
    "<center>Image taken from <a href=\"https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html\">here</a></center>\n",
    "\n",
    "Now it's time for you to put together one of the first models for object detection - **R-CNN**! \n",
    "\n",
    "This model is far from perfect, but it started the journey of exploring how to improve the ways we get ROIs for the *two-stage* models. Later in the course, we will discuss the most advanced version of this model called *Faster R-CNN* and all the things that took us from the R-CNN to Faster R-CNN.\n",
    "\n",
    "If you look at the architecture of the R-CNN, you'll notice that it's not much different from the model we had in the *naive approach* lesson. \n",
    "\n",
    "The simplified version of the R-CNN model:\n",
    "\n",
    "We start with some regions of interest. In the case of R-CNN, we use Selective Search to get these regions. After we have about 2k of them, we put them through a pre-trained CNN architecture (Originally VGG) and get predictions for each class + background.\n",
    "\n",
    "There is more to this. To learn how the whole (paper-based) implementation works, read this blog: https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html\n",
    "\n",
    "\n",
    "Now that you know the R-CNN architecture, let's build one! Since we have two classes (Airplane, background), we won't be making separate classifiers for each class, but our CNN will do the whole work!\n",
    "\n",
    "\n",
    "\n",
    "### Steps:\n",
    "1. Import dependencies\n",
    "2. Selective Search data generator and data generation\n",
    "3. Model definition and training\n",
    "4. Prediction loop\n",
    "\n",
    "### Topics covered and learning objectives\n",
    "- Intersection over Union (IoU)\n",
    "- Object detection concept\n",
    "\n",
    "### Time estimates:\n",
    "- Reading/Watching materials: 15min\n",
    "- Exercises: 1h\n",
    "<br><br>\n",
    "- **Total**: ~1h\n",
    "\n",
    "**This time does not include execution time!**\n",
    "\n",
    "## Implement dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8aafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PurePath, Path\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAvgPool2D\n",
    "\n",
    "# Importing custom project-based utils\n",
    "from utils import IoU, data_loader\n",
    "from tests import selective_search_generator_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e093e",
   "metadata": {},
   "source": [
    "## Loading the datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb59b1",
   "metadata": {},
   "source": [
    "### Exercise 1: Update the sliding window generator to use selective search\n",
    "\n",
    "You'll find the code for the sliding window data generator used in the **naive approach to object detection** lesson in the code below. Your task is to fill a couple of lines, so it uses the selective search algorithm instead.\n",
    "\n",
    "NOTE: *selective_search* is already given as an argument of the function. Use it inside of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3efd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search_training_data(img_obj,\n",
    "                                   selective_search,\n",
    "                                   img_size=(224, 224),\n",
    "                                   number_of_samples=10,\n",
    "                                   number_of_regions=2000):\n",
    "\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "\n",
    "    img = img_obj['img']\n",
    "\n",
    "    #YOUR CODE HERE: Addd selective search here\n",
    "    selective_search.setBaseImage(img)\n",
    "    selective_search.switchToSelectiveSearchFast()\n",
    "    ssresults = selective_search.process()\n",
    "\n",
    "    positive_samples=0\n",
    "    negative_samples=0\n",
    "    if len(ssresults) > 0:\n",
    "        number_of_regions = np.minimum(len(ssresults), number_of_regions)\n",
    "\n",
    "        for i in range(number_of_regions):\n",
    "            x,y,w,h = ssresults[i]\n",
    "            proposed_region = {\"x1\":x,\n",
    "                               \"x2\":x+w,\n",
    "                               \"y1\":y,\n",
    "                               \"y2\":y+h}\n",
    "\n",
    "            for obj in img_obj['objects']:\n",
    "\n",
    "                iou = IoU(obj, proposed_region)\n",
    "\n",
    "                # Generating positive samples\n",
    "                if positive_samples < number_of_samples:\n",
    "                    if iou > 0.7:\n",
    "                        proposal = cv2.resize(img[y:y+h,x:x+w],\n",
    "                                              img_size, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                        training_images.append(proposal)\n",
    "                        training_labels.append(1)\n",
    "                        positive_samples += 1\n",
    "\n",
    "                # Generating negative samples\n",
    "                if negative_samples < number_of_samples:\n",
    "                    if iou < 0.3:\n",
    "                        proposal = cv2.resize(img[y:y+h,x:x+w],\n",
    "                                              img_size, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                        training_images.append(proposal)\n",
    "                        training_labels.append(0)\n",
    "                        negative_samples += 1\n",
    "\n",
    "    else:\n",
    "        print(\"No regions found\")\n",
    "\n",
    "    return np.array(training_images), np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afdf1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:green>IMPLEMENTATION IS CORRECT! GOOD JOB!</text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST YOUR TRAINING SET GENERATOR\n",
    "selective_search_generator_TEST(selective_search_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484607e",
   "metadata": {},
   "source": [
    "### Exercise 2: Write the training set generation loop\n",
    "\n",
    "In the cell below, you have two empty lists, **training_images** and **training_labels** your task is to define the Selective Search algorithm and write a custom for loop that calls **selective_search_training_data** for each item in the **train_data** dictionary.\n",
    "\n",
    "Append all results to those two empty lists \n",
    "\n",
    "\n",
    "**IMPORTANT NOTE**: My implementation took about **1h 30min** to complete. Be cautious. A longer execution time is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = []\n",
    "training_labels = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "\n",
    "for key in tqdm(train_data.keys()):\n",
    "    obj = train_data[key]\n",
    "    imgs, labels = selective_search_training_data(obj, ss)\n",
    "    training_images.append(imgs)\n",
    "    training_labels.append(labels)\n",
    "\n",
    "\n",
    "# DON'T CHANGE CODE BELOW THIS LINE\n",
    "X = np.vstack(training_images)\n",
    "y = np.hstack(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5112035",
   "metadata": {},
   "source": [
    "### Exercise 3: Split data to training and testing parts using sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d98242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0875c92",
   "metadata": {},
   "source": [
    "### Exercise 4: Define VGG16 network \n",
    "\n",
    "Define the VGG16 Network trained on the **imagenet** dataset. The network should *not* include the top part.\n",
    "\n",
    "After you define it, make sure to freeze the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb4075",
   "metadata": {},
   "source": [
    "### Exercise 5: Define the custom head part of the network\n",
    "\n",
    "Define the custom header with; it's up to you what architecture you want to use, the only important part is to have the output layer defined as - \n",
    "\n",
    "`Dense(1, activation=\"sigmoid\")`\n",
    "\n",
    "Example of the custom head I've used and achieved decent accuracy:\n",
    "\n",
    "- Based model\n",
    "- Start with GlobaAvgPool2D layer \n",
    "- Output layer \n",
    "\n",
    "This model achieved about ~97% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34724a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "flattened_features = GlobalAvgPool2D()(base_model.output)\n",
    "predictions = Dense(1, activation=\"sigmoid\")(flattened_features)\n",
    "model = Model(inputs=base_model.inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e399",
   "metadata": {},
   "source": [
    "### Exercise 6: Compile and train the model\n",
    "\n",
    "While compiling the model, make sure to set `metrics=['acc']` and loss to `binary_crossentropy`. The rest of the arguments are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ce5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6095d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs= 10, validation_data=(X_test, y_test), callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353829d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data['img1.png']['img'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a604d",
   "metadata": {},
   "source": [
    "# Prediction time!\n",
    "\n",
    "### Exercise 7: Write a custom code to make predictions with the trained model\n",
    "\n",
    "The following exercise is writing the custom prediction code to make AND visualize outputs from your model.\n",
    "\n",
    "Here are the pointers to keep in mind:\n",
    "- Take images from the **test_data**\n",
    "- Run Selective Search on top of an image\n",
    "- Selective Search can generate a few hundred proposals to thousands! You don't want to check all of them. Make sure this is a constant!\n",
    "- Each proposal should be 224x224 because of the model\n",
    "- Run model on top of the proposal\n",
    "- Model will produce a confidence score ranging from 0 to 1, don't visualize all the rectangles but only the most confident once! The number that worked for me was *0.95*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_regions = 2000\n",
    "img_name = np.random.choice(list(test_data.keys()))\n",
    "print(img_name)\n",
    "random_img = test_data[img_name]['img']\n",
    "\n",
    "ss.setBaseImage(random_img)\n",
    "ss.switchToSelectiveSearchFast()\n",
    "ssresults = ss.process()\n",
    "\n",
    "number_of_regions = min(len(ssresults), number_of_regions)\n",
    "for i in tqdm(range(number_of_regions)):\n",
    "    x, y, w, h = ssresults[i]\n",
    "    proposal = cv2.resize(random_img[y:y+h, x:x+w], (224, 224), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    img = np.expand_dims(proposal, axis=0)\n",
    "    out= model.predict(img)\n",
    "    if out[0][0] > 0.95:\n",
    "        cv2.rectangle(random_img, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(random_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48829f46",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "Fantastic work so far! We got a pretty decent model with only the ROI generation change, but you've probably seen some predictions overlap and are either smaller or larger than the object we are looking for.\n",
    "\n",
    "There are a couple of things that we can do about this!\n",
    "- We can apply a technique called Hard-Negative mining to find negative samples and re-trained the model \n",
    "- Apply a technique called Non-Maximum suppression to merge overlapping predictions\n",
    "\n",
    "Let's talk about these two techniques in the following two lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300f5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
